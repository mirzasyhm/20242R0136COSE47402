{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e76c000-a0c2-4819-8b1b-f1f2f3006d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: fastparquet in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2024.11.0)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastparquet) (2.9.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastparquet) (2024.9.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastparquet) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5b13584-b117-45b1-9f13-3a7d0935f79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2023.11.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f932983f-a140-40c4-a637-f97ecae439e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2022-01.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2022-01.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2022-02.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2022-02.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2022-03.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2022-03.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2022-04.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2022-04.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2022-05.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2022-05.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2022-06.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2022-06.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2022-07.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2022-07.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2022-08.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2022-08.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2022-09.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2022-09.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2022-10.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2022-10.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2022-11.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2022-11.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2022-12.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2022-12.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2023-01.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2023-01.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2023-02.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2023-02.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2023-03.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2023-03.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2023-04.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2023-04.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2023-05.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2023-05.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2023-06.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2023-06.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2023-07.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2023-07.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2023-08.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2023-08.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2023-09.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2023-09.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2023-10.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2023-10.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2023-11.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2023-11.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2023-12.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2023-12.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2024-01.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2024-01.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2024-02.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2024-02.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2024-03.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2024-03.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2024-04.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2024-04.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2024-05.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2024-05.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2024-06.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2024-06.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2024-07.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2024-07.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2024-08.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2024-08.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2024-09.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2024-09.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2024-10.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2024-10.parquet\n",
      "Fetching data from: https://storage.data.gov.my/pricecatcher/pricecatcher_2024-11.parquet\n",
      "Successfully saved to: dataset\\pricecatcher_2024-11.parquet\n",
      "Data download completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to generate URLs for each month\n",
    "def generate_price_data_urls(start_date, end_date):\n",
    "    urls = []\n",
    "    current = start_date.replace(day=1)\n",
    "    while current <= end_date:\n",
    "        url = f\"https://storage.data.gov.my/pricecatcher/pricecatcher_{current.strftime('%Y-%m')}.parquet\"\n",
    "        urls.append(url)\n",
    "        # Move to the next month\n",
    "        if current.month == 12:\n",
    "            current = current.replace(year=current.year + 1, month=1)\n",
    "        else:\n",
    "            current = current.replace(month=current.month + 1)\n",
    "    return urls\n",
    "\n",
    "# Define the date range for price data\n",
    "start_date = datetime(2022, 1, 1)\n",
    "end_date = datetime(2024, 11, 1)  # Assuming data is up to November 2024\n",
    "\n",
    "# Generate all URLs\n",
    "price_data_urls = generate_price_data_urls(start_date, end_date)\n",
    "\n",
    "# Create 'dataset' directory if it doesn't exist\n",
    "dataset_dir = 'dataset'\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# Fetch and save each Parquet file\n",
    "for url in price_data_urls:\n",
    "    try:\n",
    "        print(f\"Fetching data from: {url}\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        \n",
    "        # Extract filename from URL\n",
    "        filename = url.split('/')[-1]\n",
    "        file_path = os.path.join(dataset_dir, filename)\n",
    "        \n",
    "        # Write content to file in chunks to handle large files efficiently\n",
    "        with open(file_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:  # Filter out keep-alive chunks\n",
    "                    f.write(chunk)\n",
    "        \n",
    "        print(f\"Successfully saved to: {file_path}\")\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred for {url}: {http_err}\")\n",
    "    except requests.exceptions.ConnectionError as conn_err:\n",
    "        print(f\"Connection error occurred for {url}: {conn_err}\")\n",
    "    except requests.exceptions.Timeout as timeout_err:\n",
    "        print(f\"Timeout error occurred for {url}: {timeout_err}\")\n",
    "    except Exception as err:\n",
    "        print(f\"An error occurred for {url}: {err}\")\n",
    "\n",
    "print(\"Data download completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35eb7bbc-5d2b-4619-a4cf-051a93b48023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastparquet in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2024.11.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting dask[complete]\n",
      "  Downloading dask-2024.11.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: click>=8.1 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[complete]) (8.1.7)\n",
      "Collecting cloudpickle>=3.0.0 (from dask[complete])\n",
      "  Downloading cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[complete]) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[complete]) (23.2)\n",
      "Collecting partd>=1.4.0 (from dask[complete])\n",
      "  Downloading partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[complete]) (6.0.1)\n",
      "Collecting toolz>=0.10.0 (from dask[complete])\n",
      "  Downloading toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pyarrow>=14.0.1 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[complete]) (15.0.0)\n",
      "Collecting lz4>=4.3.2 (from dask[complete])\n",
      "  Downloading lz4-4.3.3-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastparquet) (2.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastparquet) (1.26.3)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastparquet) (2.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click>=8.1->dask[complete]) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2023.4)\n",
      "Collecting locket (from partd>=1.4.0->dask[complete])\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting dask-expr<1.2,>=1.1 (from dask[complete])\n",
      "  Downloading dask_expr-1.1.19-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting bokeh>=3.1.0 (from dask[complete])\n",
      "  Downloading bokeh-3.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[complete]) (3.1.3)\n",
      "Collecting distributed==2024.11.2 (from dask[complete])\n",
      "  Downloading distributed-2024.11.2-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting msgpack>=1.0.2 (from distributed==2024.11.2->dask[complete])\n",
      "  Downloading msgpack-1.1.0-cp312-cp312-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from distributed==2024.11.2->dask[complete]) (5.9.8)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from distributed==2024.11.2->dask[complete]) (2.4.0)\n",
      "Collecting tblib>=1.6.0 (from distributed==2024.11.2->dask[complete])\n",
      "  Downloading tblib-3.0.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: tornado>=6.2.0 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from distributed==2024.11.2->dask[complete]) (6.4)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from distributed==2024.11.2->dask[complete]) (2.1.0)\n",
      "Collecting zict>=3.0.0 (from distributed==2024.11.2->dask[complete])\n",
      "  Downloading zict-3.0.0-py2.py3-none-any.whl.metadata (899 bytes)\n",
      "Requirement already satisfied: contourpy>=1.2 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bokeh>=3.1.0->dask[complete]) (1.2.0)\n",
      "Requirement already satisfied: pillow>=7.1.0 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bokeh>=3.1.0->dask[complete]) (10.2.0)\n",
      "Collecting xyzservices>=2021.09.1 (from bokeh>=3.1.0->dask[complete])\n",
      "  Downloading xyzservices-2024.9.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2>=2.10.3->dask[complete]) (2.1.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\n",
      "Downloading cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Downloading lz4-4.3.3-cp312-cp312-win_amd64.whl (99 kB)\n",
      "Downloading partd-1.4.2-py3-none-any.whl (18 kB)\n",
      "Downloading toolz-1.0.0-py3-none-any.whl (56 kB)\n",
      "Downloading dask-2024.11.2-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 10.7 MB/s eta 0:00:00\n",
      "Downloading distributed-2024.11.2-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 12.3 MB/s eta 0:00:00\n",
      "Downloading bokeh-3.6.1-py3-none-any.whl (6.9 MB)\n",
      "   ---------------------------------------- 0.0/6.9 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 2.1/6.9 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.9/6.9 MB 9.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.8/6.9 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.9/6.9 MB 8.4 MB/s eta 0:00:00\n",
      "Downloading dask_expr-1.1.19-py3-none-any.whl (244 kB)\n",
      "Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Downloading msgpack-1.1.0-cp312-cp312-win_amd64.whl (75 kB)\n",
      "Downloading tblib-3.0.0-py3-none-any.whl (12 kB)\n",
      "Downloading xyzservices-2024.9.0-py3-none-any.whl (85 kB)\n",
      "Downloading zict-3.0.0-py2.py3-none-any.whl (43 kB)\n",
      "Installing collected packages: zict, xyzservices, toolz, tblib, msgpack, lz4, locket, cloudpickle, partd, dask, bokeh, distributed, dask-expr\n",
      "Successfully installed bokeh-3.6.1 cloudpickle-3.1.0 dask-2024.11.2 dask-expr-1.1.19 distributed-2024.11.2 locket-1.0.0 lz4-4.3.3 msgpack-1.1.0 partd-1.4.2 tblib-3.0.0 toolz-1.0.0 xyzservices-2024.9.0 zict-3.0.0\n"
     ]
    }
   ],
   "source": [
    "pip install dask[complete] fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21eb8d26-62dc-49f5-b858-db11bf8f3712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (15.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.16.6 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyarrow) (1.26.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aec7771-f702-4b2d-9e70-bf8d3e5abc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DataFrame:\n",
      "[########################################] | 100% Completed | 222.99 ms\n",
      "[########################################] | 100% Completed | 305.15 ms\n",
      "[########################################] | 100% Completed | 403.77 ms\n",
      "        date  item_code  price\n",
      "0 2022-01-01          1    9.1\n",
      "1 2022-01-01          9   36.0\n",
      "2 2022-01-01         14   24.0\n",
      "3 2022-01-01         16    4.3\n",
      "4 2022-01-01         18    4.5\n",
      "[########################################] | 100% Completed | 11.68 s\n",
      "[########################################] | 100% Completed | 11.78 s\n",
      "[########################################] | 100% Completed | 11.88 s\n",
      "Grouped DataFrame:\n",
      "[########################################] | 100% Completed | 108.66 ms\n",
      "[########################################] | 100% Completed | 206.98 ms\n",
      "[########################################] | 100% Completed | 304.54 ms\n",
      "        date  item_code      price\n",
      "0 2024-03-04          1   8.423158\n",
      "1 2024-03-04       1581  26.000000\n",
      "2 2024-03-05       1933   7.000000\n",
      "3 2024-03-06          1   8.674659\n",
      "4 2024-03-07        120   4.142857\n",
      "[########################################] | 100% Completed | 204.39 ms\n",
      "[########################################] | 100% Completed | 206.17 ms\n",
      "[########################################] | 100% Completed | 304.18 ms\n",
      "Saved processed data for item_code 1 to preprocessed_data\\item_1.parquet\n",
      "[########################################] | 100% Completed | 105.30 ms\n",
      "[########################################] | 100% Completed | 104.84 ms\n",
      "[########################################] | 100% Completed | 203.44 ms\n",
      "Saved processed data for item_code 120 to preprocessed_data\\item_120.parquet\n",
      "[########################################] | 100% Completed | 104.53 ms\n",
      "[########################################] | 100% Completed | 104.11 ms\n",
      "[########################################] | 100% Completed | 203.18 ms\n",
      "Saved processed data for item_code 1593 to preprocessed_data\\item_1593.parquet\n",
      "[########################################] | 100% Completed | 106.99 ms\n",
      "[########################################] | 100% Completed | 205.60 ms\n",
      "[########################################] | 100% Completed | 303.78 ms\n",
      "Saved processed data for item_code 1933 to preprocessed_data\\item_1933.parquet\n",
      "[########################################] | 100% Completed | 104.47 ms\n",
      "[########################################] | 100% Completed | 104.55 ms\n",
      "[########################################] | 100% Completed | 203.42 ms\n",
      "Saved processed data for item_code 1581 to preprocessed_data\\item_1581.parquet\n",
      "Data preprocessing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='preprocess_logs.log',\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "# Initialize the progress bar for monitoring\n",
    "ProgressBar().register()\n",
    "\n",
    "# Define input and output directories\n",
    "input_dir = 'dataset'\n",
    "output_dir = 'preprocessed_data'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the target item codes\n",
    "target_item_codes = [1, 120, 1593, 1933, 1581]\n",
    "\n",
    "# Define the path pattern to read all Parquet files in the input directory\n",
    "parquet_files = os.path.join(input_dir, '*.parquet')\n",
    "\n",
    "# Read all Parquet files into a Dask DataFrame, selecting only relevant columns\n",
    "# Using 'pyarrow' engine\n",
    "try:\n",
    "    logging.info(\"Starting to read Parquet files using pyarrow engine.\")\n",
    "    df = dd.read_parquet(\n",
    "        parquet_files,\n",
    "        columns=['date', 'item_code', 'price'],\n",
    "        engine='pyarrow',\n",
    "        dtype={'item_code': 'int32', 'price': 'float32'},\n",
    "        assume_missing=True\n",
    "    )\n",
    "    logging.info(\"Successfully read Parquet files.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred while reading Parquet files: {e}\")\n",
    "    raise e\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(\"Initial DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Filter for the specified item codes\n",
    "logging.info(f\"Filtering for item codes: {target_item_codes}\")\n",
    "df_filtered = df[df['item_code'].isin(target_item_codes)]\n",
    "\n",
    "# Drop rows with missing values in 'date', 'item_code', or 'price'\n",
    "logging.info(\"Dropping rows with missing values in essential columns.\")\n",
    "df_filtered = df_filtered.dropna(subset=['date', 'item_code', 'price'])\n",
    "\n",
    "# Ensure correct data types\n",
    "logging.info(\"Converting 'date' to datetime and ensuring correct data types.\")\n",
    "df_filtered['date'] = dd.to_datetime(df_filtered['date'], errors='coerce')\n",
    "df_filtered = df_filtered.dropna(subset=['date'])\n",
    "df_filtered['item_code'] = df_filtered['item_code'].astype('int32')\n",
    "df_filtered['price'] = df_filtered['price'].astype('float32')\n",
    "\n",
    "# Group by 'date' and 'item_code' and calculate the average price\n",
    "logging.info(\"Grouping by 'date' and 'item_code' to calculate average price.\")\n",
    "df_grouped = df_filtered.groupby(['date', 'item_code']).agg({'price': 'mean'}).reset_index()\n",
    "\n",
    "# Persist the grouped DataFrame in memory for efficient access\n",
    "df_grouped = df_grouped.persist()\n",
    "logging.info(\"Persisted the grouped DataFrame in memory.\")\n",
    "\n",
    "# Display the first few rows of the grouped DataFrame\n",
    "print(\"Grouped DataFrame:\")\n",
    "print(df_grouped.head())\n",
    "\n",
    "# Iterate over each target item_code and save to separate Parquet files\n",
    "for item_code in target_item_codes:\n",
    "    try:\n",
    "        logging.info(f\"Processing item_code {item_code}.\")\n",
    "        # Filter the DataFrame for the current item_code\n",
    "        df_item = df_grouped[df_grouped['item_code'] == item_code]\n",
    "        \n",
    "        # Define the output file path\n",
    "        output_file = os.path.join(output_dir, f'item_{item_code}.parquet')\n",
    "        \n",
    "        # Save to Parquet using 'pyarrow' engine\n",
    "        df_item.to_parquet(\n",
    "            output_file,\n",
    "            engine='pyarrow',\n",
    "            compression='snappy',\n",
    "            write_index=False\n",
    "        )\n",
    "        \n",
    "        logging.info(f\"Saved processed data for item_code {item_code} to {output_file}\")\n",
    "        print(f\"Saved processed data for item_code {item_code} to {output_file}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while saving item_code {item_code}: {e}\")\n",
    "        print(f\"An error occurred while saving item_code {item_code}: {e}\")\n",
    "\n",
    "logging.info(\"Data preprocessing completed successfully.\")\n",
    "print(\"Data preprocessing completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c175aae8-697b-4c19-adfc-fe951f9a9e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: tabulate in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\msfal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8d0065c-4c94-4821-8a19-ab7f7f502af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+\n",
      "|   Item Code |   Record Count |\n",
      "|-------------+----------------|\n",
      "|           1 |            992 |\n",
      "|         120 |            988 |\n",
      "|        1581 |            262 |\n",
      "|        1593 |            164 |\n",
      "|        1933 |            486 |\n",
      "+-------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tabulate import tabulate  # Optional: For better-formatted tables\n",
    "\n",
    "# Define the directory containing preprocessed Parquet files\n",
    "preprocessed_dir = 'preprocessed_data'\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.isdir(preprocessed_dir):\n",
    "    raise FileNotFoundError(f\"The directory '{preprocessed_dir}' does not exist.\")\n",
    "\n",
    "# Define the pattern to match all Parquet files (e.g., item_1.parquet, item_120.parquet, etc.)\n",
    "parquet_pattern = os.path.join(preprocessed_dir, 'item_*.parquet')\n",
    "\n",
    "# Use glob to find all matching Parquet files\n",
    "parquet_files = glob.glob(parquet_pattern)\n",
    "\n",
    "# Check if any Parquet files are found\n",
    "if not parquet_files:\n",
    "    raise FileNotFoundError(f\"No Parquet files found in '{preprocessed_dir}' with pattern 'item_*.parquet'.\")\n",
    "\n",
    "# Initialize a list to store the counts\n",
    "record_counts = []\n",
    "\n",
    "# Iterate through each Parquet file\n",
    "for file_path in parquet_files:\n",
    "    try:\n",
    "        # Extract the filename from the file path\n",
    "        filename = os.path.basename(file_path)\n",
    "        \n",
    "        # Extract item_code from the filename\n",
    "        # Assumes filenames are in the format 'item_<item_code>.parquet'\n",
    "        item_code_str = filename.replace('item_', '').replace('.parquet', '')\n",
    "        \n",
    "        # Convert item_code to integer if possible\n",
    "        try:\n",
    "            item_code = int(item_code_str)\n",
    "        except ValueError:\n",
    "            # If conversion fails, keep it as a string\n",
    "            item_code = item_code_str\n",
    "        \n",
    "        # Load the Parquet file into a Pandas DataFrame\n",
    "        df = pd.read_parquet(file_path, engine='pyarrow')  # Ensure 'pyarrow' is installed\n",
    "        \n",
    "        # Count the number of records\n",
    "        count = len(df)\n",
    "        \n",
    "        # Append the result to the list\n",
    "        record_counts.append({'Item Code': item_code, 'Record Count': count})\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file '{file_path}': {e}\")\n",
    "\n",
    "# Create a DataFrame from the counts\n",
    "counts_df = pd.DataFrame(record_counts)\n",
    "\n",
    "# Optional: Sort the DataFrame by Item Code for better readability\n",
    "counts_df = counts_df.sort_values(by='Item Code').reset_index(drop=True)\n",
    "\n",
    "# Display the counts using tabulate for a formatted table (optional)\n",
    "print(tabulate(counts_df, headers='keys', tablefmt='psql', showindex=False))\n",
    "\n",
    "# Alternatively, simply print the DataFrame\n",
    "# print(counts_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35918912-8958-420d-ae43-c03fa9921faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Item Code: 1\n",
      "Loaded data for item_code 1: 992 records.\n",
      "Dropped 0 records due to missing values.\n",
      "Training+Validation records: 685\n",
      "Test records: 307\n",
      "Training records: 548\n",
      "Validation records: 137\n",
      "Saved Training set to: final_datasets\\item_1_train.parquet\n",
      "Saved Validation set to: final_datasets\\item_1_validation.parquet\n",
      "Saved Test set to: final_datasets\\item_1_test.parquet\n",
      "\n",
      "Processing Item Code: 120\n",
      "Loaded data for item_code 120: 988 records.\n",
      "Dropped 0 records due to missing values.\n",
      "Training+Validation records: 682\n",
      "Test records: 306\n",
      "Training records: 545\n",
      "Validation records: 137\n",
      "Saved Training set to: final_datasets\\item_120_train.parquet\n",
      "Saved Validation set to: final_datasets\\item_120_validation.parquet\n",
      "Saved Test set to: final_datasets\\item_120_test.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define directories\n",
    "preprocessed_dir = 'preprocessed_data'\n",
    "final_datasets_dir = 'final_datasets'\n",
    "\n",
    "# Create the final_datasets directory if it doesn't exist\n",
    "os.makedirs(final_datasets_dir, exist_ok=True)\n",
    "\n",
    "# Define target item codes\n",
    "target_item_codes = [1, 120]\n",
    "\n",
    "# Define date ranges\n",
    "training_validation_start = pd.Timestamp('2022-01-01')\n",
    "training_validation_end = pd.Timestamp('2023-12-31')\n",
    "test_start = pd.Timestamp('2024-01-01')\n",
    "\n",
    "# Optional: Define a validation split ratio (e.g., 80% training, 20% validation)\n",
    "validation_ratio = 0.2\n",
    "\n",
    "# Function to split training and validation\n",
    "def split_train_validation(df, train_end_date):\n",
    "    train_df = df[df['date'] <= train_end_date].copy()\n",
    "    validation_df = df[df['date'] > train_end_date].copy()\n",
    "    return train_df, validation_df\n",
    "\n",
    "# Iterate over each target item code\n",
    "for item_code in target_item_codes:\n",
    "    print(f\"\\nProcessing Item Code: {item_code}\")\n",
    "    \n",
    "    # Construct the file path\n",
    "    input_file = os.path.join(preprocessed_dir, f'item_{item_code}.parquet')\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"File not found: {input_file}. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Load the Parquet file into a Pandas DataFrame\n",
    "    try:\n",
    "        df = pd.read_parquet(input_file, engine='pyarrow')\n",
    "        print(f\"Loaded data for item_code {item_code}: {len(df)} records.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {input_file}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Data Cleaning\n",
    "    \n",
    "    # Ensure 'date' is datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    \n",
    "    # Drop rows with missing essential data\n",
    "    initial_count = len(df)\n",
    "    df.dropna(subset=['date', 'item_code', 'price'], inplace=True)\n",
    "    cleaned_count = len(df)\n",
    "    print(f\"Dropped {initial_count - cleaned_count} records due to missing values.\")\n",
    "    \n",
    "    # Ensure correct data types\n",
    "    df['item_code'] = df['item_code'].astype(int)\n",
    "    df['price'] = df['price'].astype(float)\n",
    "    \n",
    "    # Verify that item_code matches the current item\n",
    "    df = df[df['item_code'] == item_code]\n",
    "    \n",
    "    # Sort by date\n",
    "    df.sort_values(by='date', inplace=True)\n",
    "    \n",
    "    # Split into Training+Validation and Test sets\n",
    "    train_val_df = df[(df['date'] >= training_validation_start) & (df['date'] <= training_validation_end)].copy()\n",
    "    test_df = df[df['date'] >= test_start].copy()\n",
    "    \n",
    "    print(f\"Training+Validation records: {len(train_val_df)}\")\n",
    "    print(f\"Test records: {len(test_df)}\")\n",
    "    \n",
    "    # Further split Training+Validation into Training and Validation sets based on date\n",
    "    # For simplicity, let's split based on an 80-20 ratio within the Training+Validation period\n",
    "    # Calculate the split date\n",
    "    split_date = train_val_df['date'].quantile(0.8)\n",
    "    split_date = pd.Timestamp(split_date.floor('D'))  # Ensure it's a date without time\n",
    "    \n",
    "    train_df, validation_df = split_train_validation(train_val_df, split_date)\n",
    "    \n",
    "    print(f\"Training records: {len(train_df)}\")\n",
    "    print(f\"Validation records: {len(validation_df)}\")\n",
    "    \n",
    "    # Save the splits to separate Parquet files\n",
    "    try:\n",
    "        # Define output file paths\n",
    "        train_file = os.path.join(final_datasets_dir, f'item_{item_code}_train.parquet')\n",
    "        validation_file = os.path.join(final_datasets_dir, f'item_{item_code}_validation.parquet')\n",
    "        test_file = os.path.join(final_datasets_dir, f'item_{item_code}_test.parquet')\n",
    "        \n",
    "        # Save to Parquet with Snappy compression\n",
    "        train_df.to_parquet(train_file, engine='pyarrow', compression='snappy', index=False)\n",
    "        validation_df.to_parquet(validation_file, engine='pyarrow', compression='snappy', index=False)\n",
    "        test_df.to_parquet(test_file, engine='pyarrow', compression='snappy', index=False)\n",
    "        \n",
    "        print(f\"Saved Training set to: {train_file}\")\n",
    "        print(f\"Saved Validation set to: {validation_file}\")\n",
    "        print(f\"Saved Test set to: {test_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving splits for item_code {item_code}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edfeb54-22e1-40b5-ac7c-7d39601eea4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
